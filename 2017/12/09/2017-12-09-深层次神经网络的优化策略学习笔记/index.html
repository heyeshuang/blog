<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>林中阴影 | 学习笔记：神经网络的优化策略</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=generator content="Hugo 0.76.5"><meta name=ROBOTS content="INDEX, FOLLOW"><link href=/dist/app.css rel=stylesheet><link rel="shortcut icon" href=img/favicon.png type=image/png><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-40522664-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=stylesheet href=https://blog.heysh.xyz/lib/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous><script defer src=https://blog.heysh.xyz/lib/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script><script defer src=https://blog.heysh.xyz/lib/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false}]});});</script><script>(function(u,c){var d=document,t='script',o=d.createElement(t),s=d.getElementsByTagName(t)[0];o.src=u;if(c){o.addEventListener('load',function(e){c(e);});}
s.parentNode.insertBefore(o,s);})('https:\/\/blog.heysh.xyz\/lib\/pangu.min.js',function(){pangu.spacingPage();});</script><style type=text/css media="screen, print">@font-face{font-family:fancytitlefont;font-style:normal;font-display:swap;src:url(https://blog.heysh.xyz/fonts/%E6%9D%A8%E4%BB%BB%E4%B8%9C%E7%AB%B9%E7%9F%B3%E4%BD%93-Light.woff2)format('woff2'),url(https://blog.heysh.xyz/fonts/%E6%9D%A8%E4%BB%BB%E4%B8%9C%E7%AB%B9%E7%9F%B3%E4%BD%93-Light.woff)format('woff')}@font-face{font-family:noto serif cjk sc;font-style:normal;font-weight:300;font-display:swap;src:local('Noto Serif CJK SC Light'),local('NotoSerifCJK-Light'),url(https://blog.heysh.xyz/fonts/noto-serif-sc-v7-latin_chinese-simplified-300.woff2)format('woff2'),url(https://blog.heysh.xyz/fonts/noto-serif-sc-v7-latin_chinese-simplified-300.woff)format('woff')}@font-face{font-family:noto serif cjk sc;font-style:normal;font-weight:400;font-display:swap;src:local('Noto Serif CJK SC'),local('NotoSerifCJK-Regular'),url(https://blog.heysh.xyz/fonts/noto-serif-sc-v7-latin_chinese-simplified-regular.woff2)format('woff2'),url(https://blog.heysh.xyz/fonts/noto-serif-sc-v7-latin_chinese-simplified-regular.woff)format('woff')}@font-face{font-family:noto serif cjk sc;font-style:normal;font-weight:500;font-display:swap;src:local('Noto Serif CJK SC Medium'),local('NotoSerifCJK-Medium'),url(https://blog.heysh.xyz/fonts/noto-serif-sc-v7-latin_chinese-simplified-500.woff2)format('woff2'),url(https://blog.heysh.xyz/fonts/noto-serif-sc-v7-latin_chinese-simplified-500.woff)format('woff')}</style></head><body class="bg-gray-100 text-gray-700 font-sans"><div class="p-6 sm:p-10 md:p-16 flex flex-wrap"><header class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-1 md:order-1 max-w-2xl"><div class="z-50 bg-gray-100 bg-opacity-75 bg-opacity-custom lg:min-w-0.7 max-w-xl md:float-right md:text-right leading-loose tracking-tight md:sticky md:top-0 pt-2"><div><h2><a href=https://blog.heysh.xyz/ title=林中阴影 class="heading font-cursive icon">林中阴影</a></h2></div><h1 class=pt-2>学习笔记：神经网络的优化策略</h1><div class="flex flex-wrap justify-end pt-2"><div class="md:flex-grow-0 font-light"><a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400" href=/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0>深度学习</a>
&nbsp;&nbsp;
<a class="post-taxonomy-tag text-eucalyptus-500" href=/tags/ml>ML</a>&nbsp;/
<a class="post-taxonomy-tag text-eucalyptus-500" href=/tags/deeplearning.ai>deeplearning.ai</a></div><time class="text-eucalyptus-500 md:text-right md:flex-grow font-light pl-4" datetime=2017-12-09T16:53:50+08:00>2017-12-9 16:53</time></div><hr></div></header><main role=main class="w-full md:w-3/5 xl:w-1/2 max-w-3xl order-2 md:order-2 min-h-70vh pt-2 pb-4"><article><section class="mx-auto content"><div class=c-rich-text><blockquote><p>本文基本上是 <em>Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</em> 的知识点大纲。</p></blockquote><blockquote><p>具体的公式和理论，可以看<a href=http://binweber.top/tags/ML/>Bin Weber的博客</a>。</p></blockquote><h2 id=训练开发测试集traindevtest>训练/开发/测试集（Train/dev/test）</h2><ul><li>数据量10000之内：70/30（免去dev）或60/20/20</li><li>更多数据：保证dev/test足够（~10000）即可</li></ul><h2 id=过拟合>过拟合</h2><ul><li>引入更多训练样本</li><li>正则化（Normalization<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>）<ul><li>目的：减小||W|| ←此时熵最大，可能性最高</li><li>L2正则化：在cost function上附加一个W，使W最小</li><li>dropout：随机关闭一些节点</li></ul></li></ul><h2 id=欠拟合>欠拟合</h2><ul><li>增加神经网络的隐含层数</li><li>隐含层中的节点数</li><li>训练更长时间</li></ul><h2 id=初始化输入数据和参数>初始化输入数据和参数</h2><ul><li>输入数据标准化（Normalization<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>）：使每个维度在(-1,1)之间</li><li>权重（W，b）初始化：应保证经多级网络后总值基本不变（类比1.0001^n和0.9999^n）<ul><li>即Xavier初始化的变种，He初始化</li></ul></li></ul><h2 id=提高梯度下降速度mini-batch>提高梯度下降速度：mini-batch</h2><p>由于内存/计算单元不够（always），每次计算部分数据</p><h3 id=提高mini-batch的稳定性减小摆动>提高mini-batch的稳定性，减小摆动</h3><ul><li>将每个batch的优化结果进行指数加权平均（Momentum）</li><li>RMSPROP</li><li>Adam：综合以上两种方法</li><li>学习率衰减</li></ul><h2 id=超参数的选择>超参数的选择</h2><h3 id=需要的超参数>需要的超参数</h3><ul><li>α，学习率/learning rate</li><li>β1~0.9，β2~0.999，ε~1e-8（Adam中的参数）</li><li>层数</li><li>每层单元</li><li>学习率衰减</li><li>mini-batch 尺寸（2的整数次方，大概是512之内？）</li></ul><h3 id=选择方案>选择方案</h3><ul><li>随机</li><li>对于α和β，以对数形式随机</li></ul><div class=highlight><pre style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>r<span style=color:#555>=-</span><span style=color:#f60>4</span><span style=color:#555>*</span>np<span style=color:#555>.</span>random<span style=color:#555>.</span>rand()
a<span style=color:#555>=</span><span style=color:#f60>10</span><span style=color:#555>**</span>r
</code></pre></div><h2 id=batch-norm>Batch-Norm</h2><ul><li>对每一层的线性计算结果Z进行标准化</li><li>每层标准化的参数γ和β通过学习得到</li><li>提高收敛速度（与对初始值X的操作类似）</li></ul><blockquote><p>采用批标准化之后，尽管每一层的z还是在不断变化，但是它们的均值和方差将基本保持不变，这就使得后面的数据更加稳定，减少前面层与后面层的耦合 （via Bin Weber）</p></blockquote><h2 id=softmax>Softmax</h2><p>当分类器需要分出更多类时使用</p><h2 id=检验方程的梯度计算是否存在问题梯度检验>检验方程的梯度计算是否存在问题：梯度检验</h2><p>用计算斜率的方式近似梯度</p><h2 id=深度学习是否会陷入局部最优问题>深度学习是否会陷入局部最优问题？</h2><ul><li>由于参数很多，对所有参数都为峰值基本上不可能</li><li>相比而言，鞍点更加普遍</li></ul><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>意义似乎略有不同 <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></section></article></main><aside role=contentinfo class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-4 md:order-3 md:sticky md:bottom-0 self-end max-w-2xl"><div class="md:float-right md:text-right leading-loose tracking-tight md:mb-2"><div class="md:max-w-xs flex flex-col md:items-end"><ul class="font-serif flex-grow-0 flex justify-between flex-wrap md:flex-col"><li class="px-1 md:px-0"><a href=/post/ title="归档 page">归档</a></li><li class="px-1 md:px-0"><a href=/categories/ title="分类 page">分类</a></li><li class="px-1 md:px-0"><a href=/tags/ title="标签 page">标签</a></li><li class="px-1 md:px-0"><a href=/about/ title="关于 page">关于</a></li><li class="px-1 md:px-0"><a href=/wallet/ title="云乞讨 page">云乞讨</a></li><li class="px-1 md:px-0"><a href=/sentences/ title="内心戏 page">内心戏</a></li><li class="px-1 md:px-0"><a href=https://heysh.xyz/ title="芽 page">芽</a></li><div id=fastSearch class=m-0><input id=searchInput type=text size=10 class="bg-gray-100 focus:outline-none border-b border-gray-100 focus:border-eucalyptus-300 md:text-right
placeholder-java-500 min-w-0 max-w-xxxs" placeholder=search><ul id=searchResults class="bg-gray-200 px-2 divide-y divide-gray-400"></ul></div></ul><div class="flex flex-wrap-reverse md:justify-end content-end md:content-start justify-start items-start max-h-16"><a href=https://github.com/heyeshuang target=_blank class="github icon pl-1 text-eucalyptus-400 hover:text-java-400" title="github link" rel=noopener aria-label="follow on github——Opens in a new window"><div class="fill-current h-8 w-8"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><g><path fill="none" d="M0 0h24v24H0z"/><path fill-rule="nonzero" d="M5.883 18.653c-.3-.2-.558-.455-.86-.816a50.32 50.32.0 01-.466-.579c-.463-.575-.755-.84-1.057-.949a1 1 0 01.676-1.883c.752.27 1.261.735 1.947 1.588-.094-.117.34.427.433.539.19.227.33.365.44.438.204.137.587.196 1.15.14.023-.382.094-.753.202-1.095C5.38 15.31 3.7 13.396 3.7 9.64c0-1.24.37-2.356 1.058-3.292-.218-.894-.185-1.975.302-3.192a1 1 0 01.63-.582c.081-.024.127-.035.208-.047.803-.123 1.937.17 3.415 1.096A11.731 11.731.0 0112 3.315c.912.0 1.818.104 2.684.308 1.477-.933 2.613-1.226 3.422-1.096.085.013.157.03.218.05a1 1 0 01.616.58c.487 1.216.52 2.297.302 3.19.691.936 1.058 2.045 1.058 3.293.0 3.757-1.674 5.665-4.642 6.392.125.415.19.879.19 1.38a300.492 300.492.0 01-.012 2.716 1 1 0 01-.019 1.958c-1.139.228-1.983-.532-1.983-1.525l.002-.446.005-.705c.005-.708.007-1.338.007-1.998.0-.697-.183-1.152-.425-1.36-.661-.57-.326-1.655.54-1.752 2.967-.333 4.337-1.482 4.337-4.66.0-.955-.312-1.744-.913-2.404a1 1 0 01-.19-1.045c.166-.414.237-.957.096-1.614l-.01.003c-.491.139-1.11.44-1.858.949a1 1 0 01-.833.135A9.626 9.626.0 0012 5.315c-.89.0-1.772.119-2.592.35a1 1 0 01-.83-.134c-.752-.507-1.374-.807-1.868-.947-.144.653-.073 1.194.092 1.607a1 1 0 01-.189 1.045C6.016 7.89 5.7 8.694 5.7 9.64c0 3.172 1.371 4.328 4.322 4.66.865.097 1.201 1.177.544 1.748-.192.168-.429.732-.429 1.364v3.15c0 .986-.835 1.725-1.96 1.528a1 1 0 01-.04-1.962v-.99c-.91.061-1.662-.088-2.254-.485z"/></g></svg></div></a><a href=https://twitter.com/heyeshuang target=_blank class="twitter icon pl-1 text-eucalyptus-400 hover:text-java-400" title="twitter link" rel=noopener aria-label="follow on twitter——Opens in a new window"><div class="fill-current h-8 w-8"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><g><path fill="none" d="M0 0h24v24H0z"/><path fill-rule="nonzero" d="M15.3 5.55a2.9 2.9.0 00-2.9 2.847l-.028 1.575a.6.6.0 01-.68.583l-1.561-.212c-2.054-.28-4.022-1.226-5.91-2.799-.598 3.31.57 5.603 3.383 7.372l1.747 1.098a.6.6.0 01.034.993L7.793 18.17c.947.059 1.846.017 2.592-.131 4.718-.942 7.855-4.492 7.855-10.348.0-.478-1.012-2.141-2.94-2.141zm-4.9 2.81a4.9 4.9.0 018.385-3.355c.711-.005 1.316.175 2.669-.645-.335 1.64-.5 2.352-1.214 3.331.0 7.642-4.697 11.358-9.463 12.309-3.268.652-8.02-.419-9.382-1.841.694-.054 3.514-.357 5.144-1.55C5.16 15.7-.329 12.47 3.278 3.786c1.693 1.977 3.41 3.323 5.15 4.037 1.158.475 1.442.465 1.973.538z"/></g></svg></div></a></div><div class="text-sm text-gray-500 leading-tight a-gray">&copy;贺叶霜，<a href=https://creativecommons.org/licenses/by-sa/4.0/deed.zh>CC BY-SA</a><br>Built with Hugo and theme <a href=https://github.com/heyeshuang/hugo-theme-tokiwa>Tokiwa</a>. 838 words in this page.</div></div></div></aside><footer class="w-full md:w-3/5 xl:w-1/2 order-3 max-w-3xl md:order-4 pt-2"><hr class=double-line><div class="flex flex-wrap justify-between pb-2 leading-loose font-serif"><a class=flex-grow-0 href=/2017/12/04/2017-12-04-%E6%9C%80%E8%BF%91%E5%AF%B9%E5%8D%9A%E5%AE%A2%E7%9A%84%E6%94%B9%E5%8A%A8/><svg class="fill-current inline-block h-4 w-4" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path fill="none" d="M0 0h24v24H0z"/><path d="M7.828 11H20v2H7.828l5.364 5.364-1.414 1.414L4 12l7.778-7.778 1.414 1.414z"/></svg>最近对博客的改动</a>
<a class=flex-grow-0 href=/2017/12/16/zipf-law/>幂律分布与Zipf's Law<svg class="fill-current inline-block h-4 w-4" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path fill="none" d="M0 0h24v24H0z"/><path d="M16.172 11l-5.364-5.364 1.414-1.414L20 12l-7.778 7.778-1.414-1.414L16.172 13H4v-2z"/></svg></a></div><div></div><hr><div class=pb-2><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"heysh"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><hr></footer><script src=/dist/app.js></script><script src=/lib/fuse.min.js></script><script src=/lib/fastsearch.js></script></div></body></html>