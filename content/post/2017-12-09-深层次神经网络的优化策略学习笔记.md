---
author : "HeYSH"
type : "post"
tags :
    - ML
    - "deeplearning.ai"
categories :
    - 深度学习
title: "学习笔记：神经网络的优化策略"
date: 2017-12-09T16:53:50+08:00
draft: false
---



> 本文基本上是 *Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization* 的知识点大纲。

>具体的公式和理论，可以看[Bin Weber的博客](http://binweber.top/tags/ML/)。

## 训练/开发/测试集（Train/dev/test）
- 数据量10000之内：70/30（免去dev）或60/20/20
- 更多数据：保证dev/test足够（~10000）即可

## 过拟合
- 引入更多训练样本
- 正则化（Normalization[^1]）
    - 目的：减小||W|| ←此时熵最大，可能性最高
    - L2正则化：在cost function上附加一个W，使W最小
    - dropout：随机关闭一些节点

## 欠拟合
- 增加神经网络的隐含层数
- 隐含层中的节点数
- 训练更长时间

## 初始化输入数据和参数
- 输入数据标准化（Normalization[^1]）：使每个维度在(-1,1)之间
- 权重（W，b）初始化：应保证经多级网络后总值基本不变（类比1.0001^n和0.9999^n）
    - 即Xavier初始化的变种，He初始化

## 提高梯度下降速度：mini-batch
由于内存/计算单元不够（always），每次计算部分数据
### 提高mini-batch的稳定性，减小摆动
- 将每个batch的优化结果进行指数加权平均（Momentum）
- RMSPROP
- Adam：综合以上两种方法
- 学习率衰减

## 超参数的选择
### 需要的超参数
- α，学习率/learning rate
- β1~0.9，β2~0.999，ε~1e-8（Adam中的参数）
- 层数
- 每层单元
- 学习率衰减
- mini-batch 尺寸（2的整数次方，大概是512之内？）

### 选择方案
- 随机
- 对于α和β，以对数形式随机

```python
r=-4*np.random.rand()
a=10**r
```

## Batch-Norm
- 对每一层的线性计算结果Z进行标准化
- 每层标准化的参数γ和β通过学习得到
- 提高收敛速度（与对初始值X的操作类似）

> 采用批标准化之后，尽管每一层的z还是在不断变化，但是它们的均值和方差将基本保持不变，这就使得后面的数据更加稳定，减少前面层与后面层的耦合 （via Bin Weber）

## Softmax
当分类器需要分出更多类时使用

## 检验方程的梯度计算是否存在问题：梯度检验
用计算斜率的方式近似梯度

## 深度学习是否会陷入局部最优问题？
- 由于参数很多，对所有参数都为峰值基本上不可能
- 相比而言，鞍点更加普遍

[^1]: 意义似乎略有不同
